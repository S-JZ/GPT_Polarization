## Installation

Create a virtual environment and install the required libraries. \
Python Version: 3.10.4
```
$ pip install -r requirements.txt
```


## Overview

### File Structure
```
GPT_Polarization/
|-- glove.6B/
|   |-- glove.6B.300d.word2vec.txt
|-- results/
|   |-- average_scores.csv
|   |-- cosine_similarity.csv
|   |-- similarity_heatmap.png
|   |-- scores.csv
|   |-- ... (other CSV files)
|-- dataset/
|   |-- data
|       |-- FAR_LEFT.txt
|       |-- FAR_RIGHT.txt
|       |-- ... (other dataset files)
|   |-- jsonl/
|       |-- far_left.jsonl
|       |-- far_right.jsonl
|       |-- ... (other JSONL files)
|-- ... (python scripts)

```

### **create_jsonl.py**

1. Creates a jsonl file for each text file in the path directory. The jsonl file contains a message with the following format:
    - System Prompt: Constant SYSTEM_PROMPT
    - User Prompt: USER_PROMPT + text with a percent of words masked
    - Assistant Prompt: text

2. Detailed overview of each function in the file:
    1. **num_tokens_from_string(string, encoding_name)**
        - Returns the number of tokens in a text string.
    2. **mask_words_in_text(text, percent=0.15)**
        - Returns the text with a percent of words masked.
    3. **read_text_file(filename)**
        - Reads a text file and returns a list of strings.
    4. **write_jsonl(filename, message)**
        - Writes a message to a jsonl file.
    5. **create_jsonl(path, percent_masked=0.15)**
        - Creates a jsonl file for each text file in the path directory.
### **finetune.py**

1. Creates fine-tuning jobs on OpenAI.

2. The functions in the file and their descriptions are as follows:
    1. **create_file(filename)**
        - Creates a file on OpenAI.
    2. **create_fine_tune_job(file_id, hyperparameters={"n_epochs":10}, model="gpt-3.5-turbo-1106")**
        - Creates a fine-tuning job on OpenAI.
    3. **save_fine_tune_job(filename, job)**      
        - Saves a fine-tuning job to a file.
    4. **load_fine_tune_job(filename)**
        - Loads a fine-tuning job from a file.
    5. **get_fine_tune_job(job_id)**
        - Gets a fine-tuning job from OpenAI.
    6. **create_files(path)**
        - Creates a file for each text file in the path directory.
    7. **create_jobs(path)**
        - Creates a fine-tuning job for each file in the path directory.

### **evaluate.py**

1. This script evaluates the similarity between ideologies using the given model.

2. Each function is described below:
    1. **get_completions(ideology, sentence, model)**
        - Gets an ideology and a sentence and returns the completions for the sentence based on the ideology.
    2. **get_example(ideology)**: 
        - Takes in an ideology and returns an example of how to rank the words for each sentence based on the ideology.
    3. **get_results(model)**
        -Takes in a model name and generates completions for each ideology and sentence. The completions are saved to a csv file.
    4. **get_replacements()**
        - Takes in the csv files generated by get_results and replaces each word that is not in TARGETS with the closest word in TARGETS. The replacements are saved to a csv file.
    5. **find_closest_word(word)**
        - Takes in a word and returns the closest word in meaning from the target words using Glove. 
        - Falls back to Levenshtein's distance in case of any exception.

    6. **get_scores()** 
        - Takes in the csv files generated by get_replacements and calculates the scores for each word in each sentence. The scores are saved to a csv file.
    6. **get_average_scores()** 
        - Takes in the csv file generated by get_scores and calculates the average score for each word across all sentences. The average scores are saved to a csv file.
    7. **get_cosine_similarity()** 
        - Takes in the csv file generated by get_average_scores and calculates the cosine similarity between each pair of ideologies. The cosine similarities are saved to a csv file.
    8. **plot_similarity()** 
        - Takes in the csv file generated by get_cosine_similarity and plots a heatmap of the cosine similarities between each pair of ideologies. The heatmap is saved to a png file.

### **error_check.py**
This script is used to check the format of the dataset while running create_jsonl.py.



## How to get started?

Set the ```API_KEY``` variable in ```evaluate.py``` and ```finetune.py``` file to the GPT's API Key.

1. For creating JSONL files:
    - Make changes to the SYSTEM_PROMPT AND USER_PROMPT
    - Run ```python create_jsonl.py```

2. For finetuning: 

    - Set the appropriate models and the hyperparameters.
    - Call the create_file() function to first create all the files for fine_tuning using the jsonl files.
    - Call the create_jobs() function to create a finetuning job.

3. For evaluating: 
    - Make changes to the prompts, and run the replacements function, if required.
    - Set the appropriate model.
    - Run ```python evaluate.py```

